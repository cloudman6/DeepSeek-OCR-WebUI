#!/usr/bin/env python3
"""
GPU Resource Manager - Lazy Load + Instant Offload
æ‡’åŠ è½½ + å³ç”¨å³å¸çš„ GPU æ˜¾å­˜ç®¡ç†
"""
import time
import threading
import logging
import torch
import gc
from typing import Optional, Callable

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GPUResourceManager:
    """GPU èµ„æºç®¡ç†å™¨"""
    
    def __init__(self, idle_timeout: int = 60):
        """
        Args:
            idle_timeout: ç©ºé—²è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.idle_timeout = idle_timeout
        self.model = None  # GPU ä¸Šçš„æ¨¡åž‹
        self.model_on_cpu = None  # CPU ç¼“å­˜
        self.processor = None
        self.lock = threading.Lock()
        self.last_use_time = 0
        self.running = False
        self.monitor_thread = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ðŸŽ¯ GPU Manager initialized (device={self.device}, timeout={idle_timeout}s)")
    
    def get_model(self, load_func: Callable):
        """
        æ‡’åŠ è½½é€»è¾‘ï¼š
        1. å¦‚æžœåœ¨ GPU ä¸Š â†’ ç›´æŽ¥è¿”å›ž
        2. å¦‚æžœåœ¨ CPU ä¸Š â†’ å¿«é€Ÿè½¬ç§»åˆ° GPUï¼ˆ2-5ç§’ï¼‰
        3. å¦‚æžœæœªåŠ è½½ â†’ ä»Žç£ç›˜åŠ è½½ï¼ˆé¦–æ¬¡ 20-30ç§’ï¼‰
        """
        with self.lock:
            self.last_use_time = time.time()
            
            # æƒ…å†µ1: å·²åœ¨ GPU ä¸Š
            if self.model is not None:
                logger.info("âœ… Model already on GPU")
                return self.model, self.processor
            
            # æƒ…å†µ2: åœ¨ CPU ç¼“å­˜ä¸­ï¼Œå¿«é€Ÿè½¬ç§»
            if self.model_on_cpu is not None:
                logger.info("ðŸ”„ Moving model from CPU to GPU...")
                start = time.time()
                self.model = self.model_on_cpu.to(self.device)
                self.model_on_cpu = None
                logger.info(f"âœ… Model moved to GPU in {time.time()-start:.1f}s")
                return self.model, self.processor
            
            # æƒ…å†µ3: é¦–æ¬¡åŠ è½½
            logger.info("ðŸ“¥ Loading model from disk (first time)...")
            start = time.time()
            self.model, self.processor = load_func()
            logger.info(f"âœ… Model loaded in {time.time()-start:.1f}s")
            return self.model, self.processor
    
    def force_offload(self):
        """
        å³ç”¨å³å¸ï¼šä»»åŠ¡å®ŒæˆåŽç«‹å³è°ƒç”¨
        å°†æ¨¡åž‹ä»Ž GPU è½¬ç§»åˆ° CPUï¼Œé‡Šæ”¾æ˜¾å­˜
        """
        with self.lock:
            if self.model is not None:
                logger.info("ðŸ’¾ Offloading model to CPU...")
                start = time.time()
                self._move_to_cpu()
                logger.info(f"âœ… Model offloaded in {time.time()-start:.1f}s")
    
    def force_release(self):
        """
        å®Œå…¨é‡Šæ”¾ï¼šé•¿æœŸä¸ç”¨æ—¶è°ƒç”¨
        æ¸…ç©º GPU å’Œ CPU ç¼“å­˜
        """
        with self.lock:
            logger.info("ðŸ—‘ï¸ Releasing all resources...")
            self.model = None
            self.model_on_cpu = None
            self.processor = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            logger.info("âœ… All resources released")
    
    def _move_to_cpu(self):
        """å†…éƒ¨æ–¹æ³•ï¼šå°†æ¨¡åž‹ç§»åˆ° CPU"""
        if self.model is not None:
            self.model_on_cpu = self.model.cpu()
            self.model = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def start_monitor(self):
        """å¯åŠ¨ç›‘æŽ§çº¿ç¨‹"""
        if self.running:
            return
        
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        logger.info("ðŸ” Monitor thread started")
    
    def stop_monitor(self):
        """åœæ­¢ç›‘æŽ§çº¿ç¨‹"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        logger.info("ðŸ›‘ Monitor thread stopped")
    
    def _monitor_loop(self):
        """ç›‘æŽ§å¾ªçŽ¯ï¼šè‡ªåŠ¨å¸è½½ç©ºé—²æ¨¡åž‹"""
        while self.running:
            time.sleep(30)  # æ¯30ç§’æ£€æŸ¥
            
            # æš‚æ—¶ç¦ç”¨è‡ªåŠ¨å¸è½½ï¼Œé¿å…æŽ¨ç†è¿‡ç¨‹ä¸­è¢«å¸è½½
            # with self.lock:
            #     if self.model is not None:
            #         idle_time = time.time() - self.last_use_time
            #         
            #         if idle_time > self.idle_timeout:
            #             logger.info(f"â° Idle for {idle_time:.0f}s, auto-offloading...")
            #             self._move_to_cpu()
    
    def get_status(self) -> dict:
        """èŽ·å–å½“å‰çŠ¶æ€"""
        with self.lock:
            status = {
                "model_location": "gpu" if self.model is not None else ("cpu" if self.model_on_cpu is not None else "unloaded"),
                "idle_time": time.time() - self.last_use_time if self.last_use_time > 0 else 0,
                "device": self.device,
                "timeout": self.idle_timeout
            }
            
            if torch.cuda.is_available():
                status["gpu_memory_allocated"] = torch.cuda.memory_allocated() / 1024**2  # MB
                status["gpu_memory_reserved"] = torch.cuda.memory_reserved() / 1024**2  # MB
            
            return status
