#!/usr/bin/env python3
"""
DeepSeek-OCR Web Service with GPU Management
é›†æˆ GPU æ™ºèƒ½ç®¡ç†çš„ Web æœåŠ¡
"""
import os
import re
import tempfile
import io
import base64
import platform
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager
from pathlib import Path

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image, ImageOps
import uvicorn
import fitz

from gpu_manager import GPUResourceManager

# å…¨å±€ GPU ç®¡ç†å™¨
gpu_manager = None
backend_type = None

def detect_platform() -> str:
    """æ£€æµ‹å¹³å°"""
    force_backend = os.environ.get("FORCE_BACKEND", "").lower()
    if force_backend in ["cuda", "cpu"]:
        return force_backend
    
    try:
        import torch
        if torch.cuda.is_available():
            return "cuda"
    except ImportError:
        pass
    
    return "cpu"

def load_model_func():
    """æ¨¡å‹åŠ è½½å‡½æ•° - ä¾› GPU ç®¡ç†å™¨è°ƒç”¨"""
    from backends.cuda_backend import CUDABackend
    backend = CUDABackend()
    
    try:
        backend.load_model(source="huggingface", timeout=300)
    except:
        print("ğŸ”„ Switching to ModelScope...")
        backend.load_model(source="modelscope")
    
    return backend.model, backend.processor

@asynccontextmanager
async def lifespan(app: FastAPI):
    """å¯åŠ¨æ—¶åˆå§‹åŒ– GPU ç®¡ç†å™¨"""
    global gpu_manager, backend_type
    
    print("="*50)
    print("ğŸš€ DeepSeek-OCR with GPU Management")
    print("="*50)
    
    backend_type = detect_platform()
    
    if backend_type == "cuda":
        # åˆå§‹åŒ– GPU ç®¡ç†å™¨
        idle_timeout = int(os.environ.get("GPU_IDLE_TIMEOUT", "60"))
        gpu_manager = GPUResourceManager(idle_timeout=idle_timeout)
        gpu_manager.start_monitor()
        print(f"âœ… GPU Manager initialized (timeout={idle_timeout}s)")
    else:
        print("âš ï¸ CPU mode - GPU manager disabled")
    
    print("="*50)
    
    yield
    
    if gpu_manager:
        gpu_manager.stop_monitor()
    print("ğŸ›‘ Service shutting down...")

app = FastAPI(
    title="DeepSeek-OCR with GPU Management",
    description="OCR service with lazy loading and instant offload",
    version="4.1.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def build_prompt(mode: str, custom_prompt: str = "", find_term: str = "") -> str:
    """æ„å»ºæç¤ºè¯"""
    templates = {
        "document": "<image>\n<|grounding|>Convert the document to markdown.",
        "ocr": "<image>\n<|grounding|>OCR this image.",
        "free": "<image>\nFree OCR. Only output the raw text.",
        "figure": "<image>\nParse the figure.",
        "describe": "<image>\nDescribe this image in detail.",
        "find": "<image>\n<|grounding|>Locate <|ref|>{term}<|/ref|> in the image.",
        "freeform": "<image>\n{prompt}",
    }
    
    if mode == "find":
        return templates["find"].replace("{term}", find_term.strip() or "Total")
    elif mode == "freeform":
        return templates["freeform"].replace("{prompt}", custom_prompt.strip() or "OCR this image.")
    return templates.get(mode, templates["document"])

def clean_grounding_text(text: str) -> str:
    """æ¸…ç†æ ‡è®°"""
    cleaned = re.sub(r"<\|ref\|>(.*?)<\|/ref\|>\s*<\|det\|>\s*\[.*?\]\s*<\|/det\|>", r"\1", text, flags=re.DOTALL)
    cleaned = re.sub(r"<\|grounding\|>", "", cleaned)
    return cleaned.strip()

def parse_detections(text: str, image_width: int, image_height: int) -> List[Dict[str, Any]]:
    """è§£æè¾¹ç•Œæ¡†"""
    boxes = []
    pattern = re.compile(r"<\|ref\|>(?P<label>.*?)<\|/ref\|>\s*<\|det\|>\s*(?P<coords>\[.*?\])\s*<\|/det\|>", re.DOTALL)
    
    for m in pattern.finditer(text or ""):
        label = m.group("label").strip()
        coords_str = m.group("coords").strip()
        
        try:
            import ast
            parsed = ast.literal_eval(coords_str)
            
            if isinstance(parsed, list) and len(parsed) == 4:
                box_coords = [parsed]
            elif isinstance(parsed, list):
                box_coords = parsed
            else:
                continue
            
            for box in box_coords:
                if isinstance(box, (list, tuple)) and len(box) >= 4:
                    x1 = int(float(box[0]) / 999 * image_width)
                    y1 = int(float(box[1]) / 999 * image_height)
                    x2 = int(float(box[2]) / 999 * image_width)
                    y2 = int(float(box[3]) / 999 * image_height)
                    boxes.append({"label": label, "box": [x1, y1, x2, y2]})
        except:
            continue
    
    return boxes

@app.get("/", response_class=HTMLResponse)
async def root():
    """è¿”å› Web UI"""
    ui_file = Path(__file__).parent / "ocr_ui_modern.html"
    if ui_file.exists():
        return HTMLResponse(content=ui_file.read_text(encoding='utf-8'))
    return HTMLResponse(content="<h1>DeepSeek-OCR</h1><p>UI not found</p>")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    status = {
        "status": "healthy",
        "backend": backend_type,
        "platform": platform.system(),
        "gpu_manager": gpu_manager is not None
    }
    
    if gpu_manager:
        status.update(gpu_manager.get_status())
    
    return status

@app.get("/gpu/status")
async def gpu_status():
    """GPU çŠ¶æ€"""
    if not gpu_manager:
        raise HTTPException(status_code=400, detail="GPU manager not available")
    
    return gpu_manager.get_status()

@app.post("/gpu/offload")
async def gpu_offload():
    """æ‰‹åŠ¨å¸è½½ GPU"""
    if not gpu_manager:
        raise HTTPException(status_code=400, detail="GPU manager not available")
    
    gpu_manager.force_offload()
    return {"status": "offloaded", "message": "Model moved to CPU"}

@app.post("/gpu/release")
async def gpu_release():
    """å®Œå…¨é‡Šæ”¾èµ„æº"""
    if not gpu_manager:
        raise HTTPException(status_code=400, detail="GPU manager not available")
    
    gpu_manager.force_release()
    return {"status": "released", "message": "All resources freed"}

@app.post("/ocr")
async def ocr_endpoint(
    file: UploadFile = File(...),
    prompt_type: str = Form("document"),
    find_term: str = Form(""),
    custom_prompt: str = Form(""),
    grounding: bool = Form(False)
):
    """OCR ç«¯ç‚¹ - ä½¿ç”¨ GPU ç®¡ç†å™¨"""
    tmp_file = None
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„å›¾ç‰‡
        image_data = await file.read()
        with tempfile.NamedTemporaryFile(delete=False, suffix='.png', mode='wb') as tmp:
            tmp.write(image_data)
            tmp_file = tmp.name
        
        # è·å–å›¾ç‰‡å°ºå¯¸
        with Image.open(tmp_file) as img:
            img = ImageOps.exif_transpose(img).convert('RGB')
            orig_w, orig_h = img.size
        
        # æ„å»ºæç¤ºè¯
        prompt = build_prompt(prompt_type, custom_prompt, find_term)
        
        # æ­¥éª¤1: æ‡’åŠ è½½æ¨¡å‹
        if gpu_manager:
            model, processor = gpu_manager.get_model(load_func=load_model_func)
        else:
            # CPU æ¨¡å¼
            from backends.cpu_backend import CPUBackend
            backend = CPUBackend()
            backend.load_model()
            model, processor = backend.model, backend.processor
        
        # æ­¥éª¤2: æ¨ç†
        from backends.cuda_backend import CUDABackend
        backend = CUDABackend()
        backend.model = model
        backend.processor = processor
        text = backend.infer(prompt=prompt, image_path=tmp_file)
        
        # æ­¥éª¤3: ç«‹å³å¸è½½ï¼ˆå…³é”®ï¼ï¼‰
        if gpu_manager:
            gpu_manager.force_offload()
        
        # è§£æç»“æœ
        boxes = parse_detections(text, orig_w, orig_h) if "<|det|>" in text else []
        display_text = clean_grounding_text(text)
        if not display_text and boxes:
            display_text = ", ".join([b["label"] for b in boxes])
        
        return JSONResponse({
            "success": True,
            "text": display_text,
            "raw_text": text,
            "boxes": boxes,
            "image_dims": {"w": orig_w, "h": orig_h},
            "prompt_type": prompt_type,
            "metadata": {
                "mode": prompt_type,
                "backend": backend_type,
                "has_boxes": len(boxes) > 0,
                "gpu_managed": gpu_manager is not None
            }
        })
        
    except Exception as e:
        # å¼‚å¸¸æ—¶ä¹Ÿè¦å¸è½½
        if gpu_manager:
            gpu_manager.force_offload()
        
        import traceback
        print(f"âŒ Error:\n{traceback.format_exc()}")
        return JSONResponse({"success": False, "error": str(e)}, status_code=500)
        
    finally:
        if tmp_file and os.path.exists(tmp_file):
            os.remove(tmp_file)

@app.post("/pdf-to-images")
async def pdf_to_images_endpoint(file: UploadFile = File(...)):
    """PDF è½¬å›¾ç‰‡"""
    tmp_file = None
    
    try:
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Must be PDF")
        
        pdf_data = await file.read()
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf', mode='wb') as tmp:
            tmp.write(pdf_data)
            tmp_file = tmp.name
        
        images = []
        pdf_doc = fitz.open(tmp_file)
        zoom = 144 / 72.0
        matrix = fitz.Matrix(zoom, zoom)
        
        for page_num in range(pdf_doc.page_count):
            page = pdf_doc[page_num]
            pixmap = page.get_pixmap(matrix=matrix, alpha=False)
            img_data = pixmap.tobytes("png")
            img = Image.open(io.BytesIO(img_data)).convert('RGB')
            
            img_buffer = io.BytesIO()
            img.save(img_buffer, format='PNG', optimize=True)
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')
            
            images.append({
                "data": f"data:image/png;base64,{img_base64}",
                "name": f"page_{page_num + 1}.png",
                "width": img.size[0],
                "height": img.size[1],
                "page_number": page_num + 1
            })
        
        pdf_doc.close()
        
        return JSONResponse({
            "success": True,
            "images": images,
            "page_count": len(images),
            "original_filename": file.filename
        })
        
    except Exception as e:
        import traceback
        print(f"âŒ PDF Error:\n{traceback.format_exc()}")
        return JSONResponse({"success": False, "error": str(e)}, status_code=500)
        
    finally:
        if tmp_file and os.path.exists(tmp_file):
            os.remove(tmp_file)

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8001))
    print(f"\n{'='*50}")
    print(f"ğŸš€ DeepSeek-OCR with GPU Management")
    print(f"{'='*50}")
    print(f"ğŸ“ URL: http://0.0.0.0:{port}")
    print(f"ğŸ“š Docs: http://0.0.0.0:{port}/docs")
    print(f"{'='*50}\n")
    
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")
